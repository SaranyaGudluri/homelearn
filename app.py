# # -*- coding: utf-8 -*-
# """Untitled30.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1BhzxwactwHpIqAFFe5rflodR2975r82S
# """

# # Import Necessary Libraries
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import LabelEncoder, StandardScaler
# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
# from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor
# from sklearn.metrics import accuracy_score, r2_score
# import joblib

# # 1. Load the Data
# # Replace 'your_dataset.csv' with the path to your actual dataset
# #data = pd.read_csv('/content/skill_assessment_dataset.csv')
# data = pd.read_csv('C:/Users/Saranya Gudluri/Desktop/homelearn/skill_assessment_dataset.csv')

# # 2. Feature Selection
# # Input Features: Levels
# feature_columns = ['OOP_Level', 'CN_Level', 'DBMS_Level', 'OS_Level', 'DSA_Level']
# X = data[feature_columns]

# # Target Variables:
# # - Regression Targets: Hours
# regression_targets = ['OOP_Hours', 'CN_Hours', 'DBMS_Hours', 'OS_Hours', 'DSA_Hours']
# y_hours = data[regression_targets]

# # - Classification Targets: Strengths, Areas_to_Focus, Recommended_Resources
# classification_targets = ['Strengths', 'Areas_to_Focus', 'Recommended_Resources']
# y_class = data[classification_targets]

# # 3. Encode Categorical Variables
# # Initialize LabelEncoders for each classification target
# le_strengths = LabelEncoder()
# le_areas = LabelEncoder()
# le_resources = LabelEncoder()

# # Encode classification targets
# y_class_encoded = y_class.copy()
# y_class_encoded['Strengths'] = le_strengths.fit_transform(y_class['Strengths'])
# y_class_encoded['Areas_to_Focus'] = le_areas.fit_transform(y_class['Areas_to_Focus'])
# y_class_encoded['Recommended_Resources'] = le_resources.fit_transform(y_class['Recommended_Resources'])

# # 4. Feature Scaling
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# # 5. Split the Data into Training and Testing Sets
# # For Hours (Regression)
# X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
#     X_scaled, y_hours, test_size=0.2, random_state=37)

# # For Classification Targets
# X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
#     X_scaled, y_class_encoded, test_size=0.2, random_state=37)

# # 6. Initialize and Train the Models

# # 6.a. MultiOutputRegressor for Hours
# regressor = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=45))
# regressor.fit(X_train_reg, y_train_reg)

# # 6.b. MultiOutputClassifier for Classification Targets
# classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=45, class_weight='balanced'))
# classifier.fit(X_train_clf, y_train_clf)

# # 7. Make Predictions

# # 7.a. Predictions for Hours
# y_pred_reg = regressor.predict(X_test_reg)

# # 7.b. Predictions for Classification Targets
# y_pred_clf = classifier.predict(X_test_clf)

# # 8. Save the Trained Models and Encoders
# joblib.dump(regressor, 'multi_output_regressor.pkl')
# joblib.dump(classifier, 'multi_output_classifier.pkl')
# joblib.dump(scaler, 'scaler.pkl')
# joblib.dump(le_strengths, 'le_strengths.pkl')
# joblib.dump(le_areas, 'le_areas.pkl')
# joblib.dump(le_resources, 'le_resources.pkl')

# # 9. Define a Prediction Function
# def predict_user_profile(input_levels):
#     """
#     Predict Hours, Strengths, Areas to Focus, and Recommended Resources for a new user.

#     Parameters:
#     - input_levels: dict with keys ['OOP_Level', 'CN_Level', 'DBMS_Level', 'OS_Level', 'DSA_Level']

#     Returns:
#     - dict with predicted hours, 'Strengths', 'Areas_to_Focus', 'Recommended_Resources'
#     """
#     # Convert input data to DataFrame
#     input_df = pd.DataFrame([input_levels])

#     # Feature columns
#     feature_cols = ['OOP_Level', 'CN_Level', 'DBMS_Level', 'OS_Level', 'DSA_Level']

#     # Scale features
#     input_scaled = scaler.transform(input_df[feature_cols])

#     # Predict Hours
#     predicted_hours = regressor.predict(input_scaled)[0]
#     hours_dict = dict(zip(regression_targets, predicted_hours.astype(int)))

#     # Predict Classification Targets
#     predicted_class = classifier.predict(input_scaled)[0]
#     strengths = le_strengths.inverse_transform([predicted_class[0]])[0]
#     areas = le_areas.inverse_transform([predicted_class[1]])[0]
#     resources = le_resources.inverse_transform([predicted_class[2]])[0]

#     # Combine all predictions
#     prediction = {
#         'OOP_Hours': hours_dict['OOP_Hours'],
#         'CN_Hours': hours_dict['CN_Hours'],
#         'DBMS_Hours': hours_dict['DBMS_Hours'],
#         'OS_Hours': hours_dict['OS_Hours'],
#         'DSA_Hours': hours_dict['DSA_Hours'],
#         'Strengths': strengths,
#         'Areas_to_Focus': areas,
#         'Recommended_Resources': resources
#     }

#     return prediction

# # 10. Example Usage of the Prediction Function
# new_user_levels = {
#     'OOP_Level': 3,
#     'CN_Level': 2,
#     'DBMS_Level': 4,
#     'OS_Level': 3,
#     'DSA_Level': 5
# }

# prediction = predict_user_profile(new_user_levels)
# print("\nPrediction for the New User:")
# print(prediction)

# from flask import Flask, request, jsonify
# import joblib
# import numpy as np

# # Load the models and encoders
# regressor = joblib.load('multi_output_regressor.pkl')
# classifier = joblib.load('multi_output_classifier.pkl')
# scaler = joblib.load('scaler.pkl')
# le_strengths = joblib.load('le_strengths.pkl')
# le_areas = joblib.load('le_areas.pkl')
# le_resources = joblib.load('le_resources.pkl')

# app = Flask(__name__)

# @app.route('/predict', methods=['POST'])
# def predict():
#     data = request.json
#     input_levels = np.array([[
#         data['oops'], data['cn'], data['dbms'], data['os'], data['dsa']
#     ]])

#     # Scale the input
#     input_scaled = scaler.transform(input_levels)

#     # Predict Hours
#     predicted_hours = regressor.predict(input_scaled)[0]

#     # Predict Classification Targets
#     predicted_class = classifier.predict(input_scaled)[0]
#     strengths = le_strengths.inverse_transform([predicted_class[0]])[0]
#     areas = le_areas.inverse_transform([predicted_class[1]])[0]
#     resources = le_resources.inverse_transform([predicted_class[2]])[0]

#     result = {
#         'OOP_Hours': int(predicted_hours[0]),
#         'CN_Hours': int(predicted_hours[1]),
#         'DBMS_Hours': int(predicted_hours[2]),
#         'OS_Hours': int(predicted_hours[3]),
#         'DSA_Hours': int(predicted_hours[4]),
#         'Strengths': strengths,
#         'Areas_to_Focus': areas,
#         'Recommended_Resources': resources
#     }
#     return jsonify(result)

# if __name__ == '__main__':
#     app.run(debug=True)

# from flask import Flask, request, jsonify
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import LabelEncoder, StandardScaler
# from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
# from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor
# import joblib

# app = Flask(__name__)

# # Load the Data
# data = pd.read_csv('C:/Users/Saranya Gudluri/Desktop/homelearn/skill_assessment_dataset.csv')

# # Feature Selection
# feature_columns = ['OOP_Level', 'CN_Level', 'DBMS_Level', 'OS_Level', 'DSA_Level']
# X = data[feature_columns]

# # Target Variables:
# regression_targets = ['OOP_Hours', 'CN_Hours', 'DBMS_Hours', 'OS_Hours', 'DSA_Hours']
# y_hours = data[regression_targets]

# classification_targets = ['Strengths', 'Areas_to_Focus', 'Recommended_Resources']
# y_class = data[classification_targets]

# # Encode Categorical Variables
# le_strengths = LabelEncoder()
# le_areas = LabelEncoder()
# le_resources = LabelEncoder()

# y_class_encoded = y_class.copy()
# y_class_encoded['Strengths'] = le_strengths.fit_transform(y_class['Strengths'])
# y_class_encoded['Areas_to_Focus'] = le_areas.fit_transform(y_class['Areas_to_Focus'])
# y_class_encoded['Recommended_Resources'] = le_resources.fit_transform(y_class['Recommended_Resources'])

# # Feature Scaling
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# # Split the Data into Training and Testing Sets
# X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_scaled, y_hours, test_size=0.2, random_state=37)
# X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_scaled, y_class_encoded, test_size=0.2, random_state=37)

# # Initialize and Train the Models
# regressor = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=45))
# regressor.fit(X_train_reg, y_train_reg)

# classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=45, class_weight='balanced'))
# classifier.fit(X_train_clf, y_train_clf)

# # Save the Trained Models and Encoders
# joblib.dump(regressor, 'multi_output_regressor.pkl')
# joblib.dump(classifier, 'multi_output_classifier.pkl')
# joblib.dump(scaler, 'scaler.pkl')
# joblib.dump(le_strengths, 'le_strengths.pkl')
# joblib.dump(le_areas, 'le_areas.pkl')
# joblib.dump(le_resources, 'le_resources.pkl')

# # Define a Prediction Function
# def predict_user_profile(input_levels):
#     input_df = pd.DataFrame([input_levels])
#     feature_cols = ['OOP_Level', 'CN_Level', 'DBMS_Level', 'OS_Level', 'DSA_Level']
#     input_scaled = scaler.transform(input_df[feature_cols])
    
#     predicted_hours = regressor.predict(input_scaled)[0]
#     hours_dict = dict(zip(regression_targets, predicted_hours.astype(int)))

#     predicted_class = classifier.predict(input_scaled)[0]
#     strengths = le_strengths.inverse_transform([predicted_class[0]])[0]
#     areas = le_areas.inverse_transform([predicted_class[1]])[0]
#     resources = le_resources.inverse_transform([predicted_class[2]])[0]

#     prediction = {
#         'OOP_Hours': hours_dict['OOP_Hours'],
#         'CN_Hours': hours_dict['CN_Hours'],
#         'DBMS_Hours': hours_dict['DBMS_Hours'],
#         'OS_Hours': hours_dict['OS_Hours'],
#         'DSA_Hours': hours_dict['DSA_Hours'],
#         'Strengths': strengths,
#         'Areas_to_Focus': areas,
#         'Recommended_Resources': resources
#     }

#     return prediction

# @app.route('/predict', methods=['POST'])
# def predict():
#     data = request.get_json()
#     user_skills = {
#         'OOP_Level': data.get('oops', 0),
#         'CN_Level': data.get('cn', 0),
#         'DBMS_Level': data.get('dbms', 0),
#         'OS_Level': data.get('os', 0),
#         'DSA_Level': data.get('dsa', 0)
#     }

#     prediction = predict_user_profile(user_skills)
#     return jsonify(prediction)

# if __name__ == '__main__':
#     app.run(port=5000, debug=True)

from flask import Flask, request, jsonify
from flask_cors import CORS  # Import CORS
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor
import joblib

app = Flask(__name__)
CORS(app)  # Initialize CORS

# Load the Data
data = pd.read_csv('C:/Users/Saranya Gudluri/Desktop/homelearn/skill_assessment_dataset.csv')

# Feature Selection
feature_columns = ['OOP_Level', 'CN_Level', 'DBMS_Level', 'OS_Level', 'DSA_Level']
X = data[feature_columns]

# Target Variables:
regression_targets = ['OOP_Hours', 'CN_Hours', 'DBMS_Hours', 'OS_Hours', 'DSA_Hours']
y_hours = data[regression_targets]

classification_targets = ['Strengths', 'Areas_to_Focus', 'Recommended_Resources']
y_class = data[classification_targets]

# Encode Categorical Variables
le_strengths = LabelEncoder()
le_areas = LabelEncoder()
le_resources = LabelEncoder()

y_class_encoded = y_class.copy()
y_class_encoded['Strengths'] = le_strengths.fit_transform(y_class['Strengths'])
y_class_encoded['Areas_to_Focus'] = le_areas.fit_transform(y_class['Areas_to_Focus'])
y_class_encoded['Recommended_Resources'] = le_resources.fit_transform(y_class['Recommended_Resources'])

# Feature Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the Data into Training and Testing Sets
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_scaled, y_hours, test_size=0.2, random_state=37)
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_scaled, y_class_encoded, test_size=0.2, random_state=37)

# Initialize and Train the Models
regressor = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=45))
regressor.fit(X_train_reg, y_train_reg)

classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=45, class_weight='balanced'))
classifier.fit(X_train_clf, y_train_clf)

# Save the Trained Models and Encoders
joblib.dump(regressor, 'multi_output_regressor.pkl')
joblib.dump(classifier, 'multi_output_classifier.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(le_strengths, 'le_strengths.pkl')
joblib.dump(le_areas, 'le_areas.pkl')
joblib.dump(le_resources, 'le_resources.pkl')

# Define a Prediction Function
def predict_user_profile(input_levels):
    input_df = pd.DataFrame([input_levels])
    feature_cols = ['OOP_Level', 'CN_Level', 'DBMS_Level', 'OS_Level', 'DSA_Level']
    input_scaled = scaler.transform(input_df[feature_cols])
    
    predicted_hours = regressor.predict(input_scaled)[0]
    hours_dict = dict(zip(regression_targets, map(int, predicted_hours)))  # Convert to standard Python int

    predicted_class = classifier.predict(input_scaled)[0]
    strengths = le_strengths.inverse_transform([predicted_class[0]])[0]
    areas = le_areas.inverse_transform([predicted_class[1]])[0]
    resources = le_resources.inverse_transform([predicted_class[2]])[0]

    prediction = {
        'OOP_Hours': hours_dict['OOP_Hours'],
        'CN_Hours': hours_dict['CN_Hours'],
        'DBMS_Hours': hours_dict['DBMS_Hours'],
        'OS_Hours': hours_dict['OS_Hours'],
        'DSA_Hours': hours_dict['DSA_Hours'],
        'Strengths': strengths,
        'Areas_to_Focus': areas,
        'Recommended_Resources': resources
    }

    return prediction

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    user_skills = {
        'OOP_Level': data.get('oops', 0),
        'CN_Level': data.get('cn', 0),
        'DBMS_Level': data.get('dbms', 0),
        'OS_Level': data.get('os', 0),
        'DSA_Level': data.get('dsa', 0)
    }

    prediction = predict_user_profile(user_skills)
    return jsonify(prediction)

if __name__ == '__main__':
    app.run(port=5000, debug=True)
